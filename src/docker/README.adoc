= Docker
:toc: macro
:toc-title: Contents

toc::[]

== Dockerfile basics

The Dockerfile is a text file that contains a set of instructions for building a Docker image. You run containers from those images. Thus, the two main `docker` commands to know are:

* `docker build`: Builds a Docker image from a Dockerfile.
* `docker run`: Runs a Docker container from an image.

image::./_/docker-build-run.drawio.svg[]

.Example Dockerfile
[source,Dockerfile]
----
# Use the official Alpine Linux image.
FROM alpine:latest

# Install necessary packages.
RUN apk update && \
    apk add --no-cache bash curl git build-base ncurses-dev

# Clone btop repository and build it.
RUN git clone https://github.com/aristocratos/btop.git /opt/btop && \
    cd /opt/btop && \
    make && \
    make install

# Clean up
RUN apk del git build-base ncurses-dev && \
    rm -rf /var/cache/apk/* /opt/btop

# Run btop when the container starts.
CMD ["btop"]
----

The three main instructions in the Dockerfile are `FROM`, `RUN`, and `CMD`.

The `FROM` instruction specifies the *base image* to use for the build. Base images are typically lightweight versions of common Linux distros, with some additional system software like web servers or database servers.

Use the `:<tag>` syntax to specify a specific version of the base image. If you do not specify a tag, the default behavior is to give you the latest version of the image available from Docker Hub – but better to be explicit by using the tag `:latest` or, better still, locking to a specific version to avoid problems associated with future breaking changes.

The `RUN` instruction executes a command in the container. This is used to run commands that we would normally run directly on the command line of a physical or virtual server. The commands must be compatible with the base image. In the example above, we are using the `apk` package manager to install software packages, because this is the package manager used by the Alpine Linux base image.

Finally, the `CMD` instruction specifies the command to run when the container starts.

The `docker build` command will tell Docker to read the Dockerfile and build a container image from it, eg.:

----
docker build -t btop-image .
----

The period on the end represents the current working directory. Use an alternative path if the Dockerfile is located elsewhere.

The `-t` flag is used to tag the image with a name. This is useful for referencing the image later.

Once the container is built, to execute it we use the `docker run` command, eg:

----
docker run -rm -it btop-image
----

To run the container in detached mode, use the `-d` flag. This will run the container in the background and return the container ID.

----
docker run -rm -d btop-image
----

Use the `-rm` flag for tell Docker to throw away any old containers previously built from the specified image. The `-it` flag is used to run the container in interactive mode.

For containerized applications, more commonly the Dockerfile instructions include steps to copy some of the source files into the container. This is done using the `COPY` command. The `WORKDIR` command is equivalent to `cd`, which tends to be used before subsequent commands are `RUN` inside the container. Example:

[source,Dockerfile]
----
FROM node:19-alpine

COPY package.json /app/
COPY src /app/

WORKDIR /app

RUN npm install

CMD ["npm", "start"]
----

== Common commands

* `docker`: Running the program without any arguments will display a list of available commands and options.

=== Image commands

* `docker pull <image-name>(:<tag>)`: Pull an image from a registry (Docker Hub by default).

* `docker images` | `docker image ls`: Display all Docker images (intermediate images are hidden).

* `docker image ls -a`: Display all Docker images, including intermediate images.

* `docker image inspect <image-id>`: View the configuration of a specific image.

* `docker image rm <image-id>` or `docker rmi <image-id>`: Remove an image (any containers that are built from the image must be removed first - use `docker rm <container-id|name>`.

=== Container commands

* `docker run <image-name>(:<tag>)`: Run a container from an image. If the image is not available locally, a `docker pull` operation will be implicitly performed. The container will run in the foreground. When you exit the process, the container will stop.

* `docker run --name <container-id|name> <image-name>(:<tag>)`: Docker will generate a random name for the container, if you don't specify one using `--name`.

* `docker run -d <image-name>(:<tag>)`: Run a container from an image in the background (detached mode).

* `docker run --rm <image-name>(:<tag>)`: By default, `docker run` creates a fresh container from the specified version tag. This means you can end up with lots of unused containers lying around if you do not explicitly delete them later. Using the `--rm` option on the `run` command will automatically remove any existing containers built from the specified image. If you want to start a container that was previously created then stopped, use `docker start <container-id>` instead.

* `docker run -it <image-name>(:<tag>)`: Run the container in interactive mode, which allows you to interact with the container's shell.

* `docker run -d -p <host>:<guest> nginx:1.23`: By default, applications inside containers run in an isolated network within the container engine. To interact with running containerized applications, we need to expose the container's port (which is defined by the running application - eg. Nginx always run on port 80, and Redis on port 6379) to an available port on the host system. The `-p` (port) flag maps a container port to a host port. This is called *port binding*. For example, to map the container's port 80 to port 8080 on the host machine, use `-p 8080:80`. The container's web server will be available on port 8080 on the host machine: `localhost:8080`. We could run another containerized web server and map its port 80 to port 8081, and so on.

* `docker logs <container-id|name>`: Show logs for a running container – useful especially for containers that are started in detached mode, as the startup logs will not have been printed to the terminal.

* `docker ps` ("process status"): Display details of all containers.

* `docker ps -a`: Display details of all containers, including stopped ones.

* `docker container ls`: Display all _running_ containers.

* `docker container ls -a`: Display all containers, even if stopped.

* `docker container inspect <container-id|name>`: View the configuration of a specific container.

* `docker container rm <container-id|name>` or `docker rm <container-id|name>`: Remove a specific container.

* `docker start <container-id|name>`: Start a specific container, which has previously been created from an image. Restarted containers will retain the attributes specified when the container was created using `docker run`, such as port bindings.

* `docker stop <container-id|name>`: Stop a specific container.

* `docker restart <container-id|name>`: Restart a running container.

=== Volume commands

* `docker volume prune`: Remove all unused volumes. This does not remove data persisted locally, it just removes the volume reference.

=== Compose commands

* `docker compose up`: Start (or create, then run) all containers defined in the `docker-compose.yml` file in the current working directory.

* `docker compose up -d`: Start all defined containers in the background (detached mode).

* `docker compose up --build`: Build or rebuild the images, before starting the containers.

* `docker compose start`: Start existing containers.

* `docker compose stop`: Stop all running containers, without removing them.

* `docker compose down`: Stop and remove all containers, images, networks and volumes as defined in the `docker compose.yml` file in the current working directory.

=== Interactive mode

The `-it` option is used to run a container, or a command within a container, in interactive mode. This option is commonly used when a container is created from an image: `docker run -it <image-name>`.

`docker exec -it <container-id|name> <command>` executes a command in a container that is already running. Use `docker exec -it <container-id|name> /bin/bash` to "shell into" and get an interactive terminal (`-it`) for the container. This is equivalent to using `docker run -it <image-name>` when the container is created.

From the interactive terminal, you can navigate the filesystem in the normal way, check logs, and print out environment variables (`env`), and so on. If the container is based on a lightweight Linux distribution, you may not have access to common Linux utilities like `curl`.

Type the command `exit` to leave the interactive session.

== Networking

By default, Docker containers run in an isolated network. This means that they can't communicate with each other unless you explicitly allow it.

To allow containers to communicate with each other, you can create a user-defined network and attach containers to it. This is done using the `docker network` command.

In the following example, we set up a Node.js application in one container, and have it communicate with a MongoDB database running in another container. We will also deploy another container that provides a UI to MongoDB, MongoExpress, so we can inspect the application's database directly. The URLs we will use to access the two front-ends will be:

* `localhost:3000`
* `localhost:8081`

Pull the latest images for MongoDB and MongoExpress:

----
docker pull mongo
docker pull mongo-express
----

Type `docker network ls` to list Docker's internal networks. Docker auto-generates some networks, such as "bridge" and "host", which will be listed. To create a custom network for our databases, which we will call `mongo-network`, run:

----
docker network create mongo-network
----

We can now run the MongoDB and MongoExpress containers, attaching them to the `mongo-network` network. For MongoDB, we need to specify a container name, as we'll use this to configure the MongoExpress instance so it knows which container is the MongoDB one. We also need to pass in environment variables, which the `mongo` container uses to configuration its MongoDB instance - as https://hub.docker.com/_/mongo[documented here].

----
docker run -d \
  -p 27017:27017 \
  -e MONGO_INITDB_ROOT_USERNAME=admin \
  -e MONGO_INITDB_ROOT_PASSWORD=password \
  --name mongodb
  --net mongo-network
  mongo
----

Type `docker log <container-id>`, using the output from the previous command to capture the container ID, to check the log output. An entry near the end of the log should read "... waiting for connections ...", indicating that the MongoDB database instance started successfully.

And for MongoExpress, follow the instructions at https://hub.docker.com/_/mongo-express to specify the environment variables required to connect to the MongoDB instance. The value of the `ME_CONFIG_MONGODB_SERVER` environment variable should be the name of the container with the MongoDB instance we want to connect to.

----
docker run -d \
 -p 8081:8081 \
 -e ME_CONFIG_MONGODB_SERVER=mongodb \
 -e ME_CONFIG_MONGODB_ADMIN_USERNAME=admin \
 -e ME_CONFIG_MONGODB_ADMIN_USERPASSWORD=password \
 --net mongo-network\
 --name mongo-express \
  mongo-express
----

Check the log output again. It should reveal if the database connection was made successfully.

In your browser, go to localhost:8081 to open the Mongo Express GUI. Use the UI to create a database for your application.

If you have a JavaScript application, running on your host machine rather than in a container, you should be able to connect to the containerized MongoDB, using code similar to the following. You can write code to create, update, read, and delete records in the MongoDB database. Use MongoExpress to inspect the database and verify that the changes are being made.

[source,javascript]
----
MongoClient.connect('mongodb://admin:password@localhost:27017', function(err, client) {
  if (err) throw err;

  let db = client.db('users'); // → Name of the MongoDB table you created.
  let query = { userid: 1 };

  db.collection('users').findOne(query, function(err, result) {
    if (err) throw err;

    client.close();
    console.log(result);
  });
});
----

You can inspect the logs of the MongoDB container to see the queries being executed.

----
docker logs mongodb | tail
----

Or to stream the output:

----
docker logs mongodb -f
----

== Docker Compose

When working with multiple containers that need to communicate with each other, it's easier to use Docker Compose. This is a tool for defining and running multi-container applications.

With Docker Compose, you use a YAML file to configure your application's services, networks, and volumes. Then, with a single command, you create and start all the containers ("services") from your configuration.

The above manual configuration can be mapped to a structure like this:

.`docker-compose.yml`
[source,yaml]
----
version: '3' # version of docker-compose.yml

services:

  mongodb: # Container name
    image: mongo
    ports:
      - 27017:27017 # host:guest
    environment:
      - MONGO_INITDB_ROOT_USERNAME=admin
      - MONGO_INITDB_ROOT_PASSWORD=password

  mongo-express:
    image: mongo-express
    ports:
      - 8081:8081
    environment:
      - ME_CONFIG_MONGODB_SERVER=mongodb
      - ME_CONFIG_MONGODB_ADMIN_USERNAME=admin
      - ME_CONFIG_MONGODB_ADMIN_USERPASSWORD=password
----

By default, `docker compose` (or `docker-compose` in earlier versions of Docker) will automatically create a common network for all containers defined in a `docker-compose.yml` file, so we don't need to manually create a network.

To start the containers using the `docker-compose.yml` configuration in the current working directory, run the following command:

----
docker compose up
----

Because there will be multiple containers, you'd typically run them in detached mode:

----
docker compose -d up
----

If you change the name of the `docker-compose.yml` file, or if it exists in another directory, you will need to specify the file name in the command, eg.:

----
docker compose -d -f mongo.yml up
----

Environment variables can be used in the `docker-compose.yml` file. If a `.env` file exists in the same directory, Docker will automatically read these in. Here's an example for a Postgres database:

.`docker-compose.yml`
[source,yaml]
----
version: '3'

services:
  db:
    image: postgres:16.2
    ports:
      - ${DB_PORT}:5432
    environment:
      - POSTGRES_PASSWORD=${DB_PSWD}
      - POSTGRES_USER=${DB_USER}
      - POSTGRES_DB=${DB_NAME}
----

Below is a more advanced configuration for a distributed system with a client application, a server application, and a server-side database.

.Dockerfile for back-end application
[source,Dockerfile]
----
FROM node:18-alpine

# Copy everything from the current directory to the container's /app directory.
WORKDIR /app
COPY . .

RUN npm i -g pnpm nodemon && pnpm i

EXPOSE 4000

CMD [ "npm", "start" ]
----

.Dockerfile for front-end application
[source,Dockerfile]
----
FROM node:18-alpine

WORKDIR /app
COPY . .

RUN npm i -g pnpm nodemon && pnpm i

EXPOSE 3000

CMD [ "npm", "run", "dev" ]
----

For the MongoDB database, we don't need to create a Dockerfile for that, because we can just pull a ready-made image from Docker Hub.

.docker-compose.yml file
[source,yaml]
----
version: "3"

services:
  client:
    build: ./frontend
    container_name: react-ui
    ports:
      - "3000:3000"
    stdin_open: true
    tty: true
    depends_on:
      - api
    networks:
      - mern-network

  api:
    build: ./backend
    container_name: node-api
    restart: always
    ports:
      - "4000:4000"
    depends_on:
      - database
    networks:
      - mern-network

  database:
    image: mongo
    container_name: mongo-db
    ports:
      - "27017:27017"
    volumes:
      - /home/name/project/mongodb-backup:/data/db
    networks:
      - mern-network

networks:
  mern-network:
    driver: bridge
----

With the inter-service communication enabled using Docker's network bridge, we can configure the individual application to communicate with each other. Configurations will vary depending on how the applications are coded, but something like this:

.Front-end .env file
[source,env]
----
VITE_API_KEY="http://localhost:4000/api"
----

.Back-end .env file
[source,env]
----
MONGODB_URI=mongodb://mongo-db:27017/to-do-app
PORT=4000
----

Notice that the hostname for the MongoDB is the container name for the MongoDB service: "mongo-db".

To run all the services together, from the directory of the `docker-compose.yml` file, run:

----
docker compose up -d
----

== Volumes

Docker containers do not persist state between restarts – unlike traditional VMs.

To preserve state, you use Docker volumes. A volume is a directory on the host system that is mounted into a container. This allows the container to read and write files to the host system. Those files are persisted even if the container is stopped and restarted, and indeed even if the container is removed/deleted.

Volumes work by plugging a path in a container's virtual file system (eg. `/var/lib/mysql/data`) to a path on the host's physical file system (eg. `/home/mount/data`).

There are three types of Docker volumes:

* A direct path-to-path mapping. Use the `docker run` command with the `-v` option to map a host path to a guest path, eg. `docker run -v /home/mount/data:/var/lib/mysql/data`.

* A second option is to specify only the container directory, eg. `docker run -v /var/lib/mysql/data`. Docker will automatically allocate a path on the host system, which will be something like `/var/lib/docker/volumes/<random-hash>/_data`. This is called an anonymous volume, and each anonymous volume is scoped to a specific container. It means host files cannot be shared between containers (normally a good thing), and the volume is removed when the container is removed.

* The third option is an extension of the second, called a named volume: `docker run -v name:/var/lib/mysql/data`. Named volumes are RECOMMENDED for production, as there are added benefits to letting Docker manage the volume on the host.

Volumes can also be configured in `docker-compose.yml`. Notice that you list all the volumes at the bottom of the configuration, and then specify which containers use which volumes. For each container you can mount a volume to a different guest path. You can mount the same data to multiple containers – allowing for data sharing – even if the mount points differ between containers.

[source,yaml]
----
version: '3'

services:
  mongodb:
    image: mongo
    ports:
      - 27017:27017
    volumes:
      - db-data:/var/lib/mysql/data
  mongo-express:
    # ...

volumes:
  - db-data
----

The guest path will be specified in the documentation for the container image. For `postgres`, the data path is `/var/lib/postgresql/data`. The below example maps this to a local directory called `db-data`, which will be created in the same directory as the `docker-compose.yml` file. If this directory is under source control, you will typically want to add it to `.gitignore`. You may also need to add it to `.dockerignore` if you do not want the `COPY` command to see it.

.`docker-compose.yml`
[source,yaml]
----
version: '3'

services:
  db:
    image: postgres:16.2
    ports:
      - ${DB_PORT}:5432
    environment:
      - POSTGRES_PASSWORD=${DB_PSWD}
      - POSTGRES_USER=${DB_USER}
      - POSTGRES_DB=${DB_NAME}
    volumes:
      - ./db-data:/var/lib/postgresql/data
----

You can also use volumes to mount application code into a container, so that you can make changes to the code on your host machine and see the changes reflected in the container without needing to rebuild it. This is useful for development, but not recommended for production.

.`docker-compose.yml`
[source,yaml]
----
version: '3'

services:
  app:
    build: .
    ports:
      - 5000:5000
    command: npm run dev
    volumes:
      - .:/app            # Mount the current directory to /app in the container.
      - /app/node_modules # Prevents node_modules from being mounted.
    depends_on:
      - db
  db:
    image: postgres:16.2
    ports:
      - ${DB_PORT}:5432
    environment:
      - POSTGRES_PASSWORD=${DB_PSWD}
      - POSTGRES_USER=${DB_USER}
      - POSTGRES_DB=${DB_NAME}
    volumes:
      - ./db-data:/var/lib/postgresql/data
----

An alternative option is to use https://code.visualstudio.com/docs/devcontainers/containers[Visual Studio Dev Containers], or equivalent features of other IDEs – see the https://containers.dev/[Dev Containers Specification]). The IDE loads a container's virtual file system and thus you develop directly inside of a running container. This provides highly consistent development environments.

== Multi-stage builds

Multi-stage builds are a feature of Docker that allow you to use multiple `FROM` instructions in a single Dockerfile. This is useful where you need extra steps to prepare an image for production, eg. building production-grade artifacts, rather than running the application directly from the source code, as you would in development environments.

Example:

.Dockerfile
[source,Dockerfile]
----
# Create a base stage.
FROM node:20 as base

RUN apt install imagemagick

WORKDIR /app

COPY package*.json .

RUN npm install

# Create a dev container that has all of the source code inside of it:
FROM base as dev

COPY . .

# Create a build stage for production.
FROM dev as build

RUN npm run build

# Create a production container that only has the build artifacts,
# and not the source code.
FROM base as prod

# Run the build stage first, then copy the build artifacts.
COPY --from=build /app/dist /app
----

These stages can then be referenced from the `docker-compose.yml` file. Convention is to use different docker-compose files for different environments, eg. `docker-compose.dev.yml` and `docker-compose.prod.yml`.

.`docker-compose.dev.yml`
[source,yaml]
----
version: '3'

services:
  app:
    build:
      context: .
      target: dev # Target stage.
    ports:
      - 5000:5000
    command: npm run dev
    volumes:
      - .:/app
      - /app/node_modules
    depends_on:
      - db
  db:
    image: postgres:16.2
    ports:
      - ${DB_PORT}:5432
    environment:
      - POSTGRES_PASSWORD=${DB_PSWD}
      - POSTGRES_USER=${DB_USER}
      - POSTGRES_DB=${DB_NAME}
    volumes:
      - ./db-data:/var/lib/postgresql/data
----

.`docker-compose.prod.yml`
[source,yaml]
----
version: '3'

services:
  app:
    build:
      context: .
      target: prod # Target stage.
    ports:
      - 5000:5000
    # Run the app, rather than the dev environment:
    command: node src/index.js
    # These volumes are not needed in the prod environment.
    # volumes:
    #   - .:/app
    #   - /app/node_modules
    depends_on:
      - db
    # Environment variables need to be captured from the actual environment,
    # not the `.env file:
    environment:
      - NODE_ENV=production
      - DB_HOST=${DB_HOST}
      - DB_USER=${DB_USER}
      - DB_PASSWORD=${DB_PSWD}
      - DB_NAME=${DB_NAME}
      - DB_PORT=${DB_PORT}
      - DATABASE_URL=${DB_URL}
  db:
    image: postgres:16.2
    ports:
      - ${DB_PORT}:5432
    environment:
      - POSTGRES_PASSWORD=${DB_PSWD}
      - POSTGRES_USER=${DB_USER}
      - POSTGRES_DB=${DB_NAME}
    volumes:
      - ./db-data:/var/lib/postgresql/data
----

You can simplify these configurations by extracting the common `db` service to a base configuration, and then having the dev and prod configurations extend that base configuration.

.`docker-compose.base.yml`
[source,yaml]
----
version: '3'

services:
  db:
    image: postgres:16.2
    ports:
      - ${DB_PORT}:5432
    environment:
      - POSTGRES_PASSWORD=${DB_PSWD}
      - POSTGRES_USER=${DB_USER}
      - POSTGRES_DB=${DB_NAME}
    volumes:
      - ./db-data:/var/lib/postgresql/data
----

.`docker-compose.dev.yml`
[source,yaml]
----
version: '3'

services:
  app:
    build:
      context: .
      target: dev
    ports:
      - 5000:5000
    command: npm run dev
    volumes:
      - .:/app
      - /app/node_modules
    depends_on:
      - db
  db:
    extends:
      file: docker-compose.base.yml
      service: db

----

.`docker-compose.prod.yml`
[source,yaml]
----
version: '3'

services:
  app:
    build:
      context: .
      target: prod
    ports:
      - 5000:5000
    command: node src/index.js
    depends_on:
      - db
    environment:
      - NODE_ENV=production
      - DB_HOST=${DB_HOST}
      - DB_USER=${DB_USER}
      - DB_PASSWORD=${DB_PSWD}
      - DB_NAME=${DB_NAME}
      - DB_PORT=${DB_PORT}
      - DATABASE_URL=${DB_URL}
  db:
    extends:
      file: docker-compose.base.yml
      service: db
----

Usage:

----
docker compose -f docker-compose.prod.yml up --build
----

== Shortcut scripts

The following are handy scripts to make it easier to work with Docker containers.

=== `install`

Add this script to the root directory of your project:

.`install`
[source,bash]
----
#!/usr/bin/env bash

set -e

docker compose down
docker compose build --pull
docker compose up -d
----

You can then run `./install` to build and start the containers defined in `docker-compose.yml`.

Extend the script to declare environment variables that are required by `docker-compose.yml`, eg:

[source,bash]
----
#!/usr/bin/env bash

set -e

export HOST_IP=`ip -4 addr show scope global dev docker0 | grep inet | awk '{print \$2}' | cut -d / -f 1`
export HOST_UID=$UID
export HOST_GID=$(id -g)
export HOST_NAME=${hostname}

docker compose down
docker compose build --pull
docker compose up -d
----

=== `run`

Add the following script in the root directory of your project:

.`run`
[source,bash]
----
#!/usr/bin/env bash

set -e

docker compose exec -it <container-name> "${@:1}"
----

You can then run `./run <command> <args...>` to execute a command in the named container.

Additional scripts can be created as shortcuts for running specific commands. For example, to run PHP's `composer`:

.`composer`
[source,bash]
----
#!/usr/bin/env bash

./run composer "${@:1}"
----

And a further shortcut to update dependencies using Composer:

.`update`
[source,bash]
----
#!/usr/bin/env bash

./composer install
----

== `start`

The following script starts a container and keeps it running in the background indefinitely -
like a daemon process. `/bin/sh -c "sleep infinity"` is run inside the container. This keeps the
container running indefinitely by executing a shell that sleeps forever. This setup is useful for
creating a persistent environment in which you can execute one-off commands (eg. build steps)
quickly.

.`start`
[source,sh]
----
#!/bin/sh

IMAGE=<image-name>

exec docker run -d --rm --name <container-name> -i -t -v $PWD:/data "$IMAGE" /bin/sh -c "sleep infinity"
----
